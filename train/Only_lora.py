import torch
import os
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, TaskType

# ================= é…ç½®åŒºåŸŸ =================
# --- 1. é…ç½®å‚æ•° ---
model_id = "/root/autodl-tmp/attentioncode/qwen2.5-coder-7B"
data_file = "/root/autodl-tmp/attentioncode/data/qwen2.5-coder-7B/mixed_training_data.jsonl"
output_dir = "./qwen2.5-coder-7B/qwen_lora_output1"

# è®­ç»ƒå‚æ•°
MAX_LENGTH = 8192
BATCH_SIZE = 1
GRAD_ACCUM = 8  # å»ºè®®å¢åŠ ç´¯ç§¯æ­¥æ•°ï¼Œæ··åˆä»»åŠ¡ä¸‹å¤§ Batch æ›´ç¨³
NUM_EPOCHS = 1
LEARNING_RATE = 1e-5

# [é‡è¦] æ¢å¤ç‰¹æ®Š Token å®šä¹‰
special_tokens = [
    "[GEN_GLOBAL_PLAN]", "[Algorithm]", "[GEN_PLAN]", "[GEN_CODE]",
    "[Record]", "[Record analysis]", "[PLAN_VERIFICATION]", "[Results Compare]"
    "[START_PLAN]","[END_PLAN]","[START_PROBLEM]","[END_PROBLEM]"
]


# ===========================================

def main():
    # --- 1. åŠ è½½ Tokenizer ---
    print(f"æ­£åœ¨åŠ è½½ Tokenizer: {model_id} ...")
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # [ä¿®å¤1] æ·»åŠ ç‰¹æ®Š Token
    num_added_toks = tokenizer.add_tokens(special_tokens)
    print(f"å·²æ·»åŠ  {num_added_toks} ä¸ªç‰¹æ®Š Token")

    # --- 2. æ ¸å¿ƒï¼šå¤šè½®å¯¹è¯æ•°æ®å¤„ç†å‡½æ•° ---
    def process_func(example):
        input_ids = []
        labels = []
        for i, msg in enumerate(example['messages']):
            if i == 0:
                prev_ids = []
            else:
                prev_ids = tokenizer.apply_chat_template(
                    example['messages'][:i],
                    tokenize=True,
                    add_generation_prompt=False
                )
            curr_ids = tokenizer.apply_chat_template(
                example['messages'][:i + 1],
                tokenize=True,
                add_generation_prompt=False
            )
            new_token_ids = curr_ids[len(prev_ids):]
            input_ids.extend(new_token_ids)
            if msg['role'] == 'assistant':
                labels.extend(new_token_ids)
            else:
                labels.extend([-100] * len(new_token_ids))

        if len(input_ids) > MAX_LENGTH:
            input_ids = input_ids[:MAX_LENGTH]
            labels = labels[:MAX_LENGTH]

        return {
            "input_ids": input_ids,
            "attention_mask": [1] * len(input_ids),
            "labels": labels
        }

    # --- 3. åŠ è½½å¹¶å¤„ç†æ•°æ® ---
    print("æ­£åœ¨åŠ è½½å¹¶å¤„ç†æ•°æ®...")
    dataset = load_dataset("json", data_files=data_file, split="train")

    # [å¯é€‰] å¦‚æœä½ éœ€è¦ä¹‹å‰æåˆ°çš„â€œè‡ªåŠ¨æ·»åŠ åœæ­¢ç¬¦â€åŠŸèƒ½ï¼Œè¯·åœ¨è¿™é‡Œæ’å…¥ dataset.map(add_eos_token_batch)

    tokenized_dataset = dataset.map(process_func, remove_columns=dataset.column_names, num_proc=4)
    print(f"æ•°æ®å¤„ç†å®Œæˆï¼Œå…± {len(tokenized_dataset)} æ¡æ ·æœ¬")

    # --- 4. åŠ è½½æ¨¡å‹ ---
    print("æ­£åœ¨åŠ è½½æ¨¡å‹ (BFloat16 + SDPA)...")
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        attn_implementation="sdpa"
    )

    # [ä¿®å¤2] è°ƒæ•´ Embedding å¤§å°
    model.resize_token_embeddings(len(tokenizer))

    model.gradient_checkpointing_enable()
    model.enable_input_require_grads()

    # --- 5. é…ç½® LoRA ---
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=64,
        lora_alpha=128,
        lora_dropout=0.05,
        # [å»ºè®®] å¢åŠ  MLP å±‚ (gate/up/down) æ•ˆæœé€šå¸¸æ›´å¥½
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        # [ä¿®å¤3] å¿…é¡»ä¿å­˜ embed_tokensï¼Œå› ä¸ºæˆ‘ä»¬åŠ äº†æ–°è¯ï¼
        modules_to_save=["embed_tokens", "lm_head"]
    )

    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # --- 6. è®­ç»ƒå‚æ•° ---
    args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUM,
        logging_steps=5,
        save_total_limit=1,
        num_train_epochs=NUM_EPOCHS,
        learning_rate=LEARNING_RATE,
        save_strategy="epoch",
        fp16=False,
        bf16=True,
        group_by_length=True,
        dataloader_num_workers=2,
        report_to="none"
    )

    # --- 7. å¼€å§‹è®­ç»ƒ ---
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_dataset,
        tokenizer=tokenizer,
        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)
    )

    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    trainer.train()

    # --- 8. ä¿å­˜æ¨¡å‹ ---
    print(f"âœ… è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨ä¿å­˜åˆ° {output_dir}")
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)


if __name__ == "__main__":
    main()